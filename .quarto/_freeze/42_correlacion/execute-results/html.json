{
  "hash": "2fc0908d8694cc12657109e2e3e4a1bc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Análisis de correlación\"\n---\n\n```{=html}\n<style>\nbody {\ntext-align: justify}\n</style>\n```\n\n\n\n\n\n\n\n## 1. En R\n\nLos análisis de correlación buscan averiguar si existe relación entre dos variables continuas.\n\n### **Covarianza:**\n\n``` r\ncov(datos$var1, datos$var2)\n```\n\nEj. Se quiere comprobar la relación entre en índice de corrupción de un país (*cpi*, donde 0=más corrupto; 10=menos corrupto) y el índice de desarrollo humano (*hdi*, donde 0=más bajo; 1=más alto).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncov(data$cpi, data$hdi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2435116\n```\n\n\n:::\n:::\n\n\n\n### **Correlación de Pearson:**\n\nLa función para calcular la correlación es `cor()` *(funciona igual que el comando cov)*. Sin embargo, para poder interpretar más adecuadamente los resultados de la correlación conviene realizar un test para comprobar si dicha correlación es estadísticamente significativa. Las hipótesis de este test son:\n\n-   H~0~= la correlación es igual a 0, así que no hay relación\n-   H~1~= la correlación es significativamente distinta de 0\n\n``` r\ncor.test(datos$var1, datos$var2)\n```\n\nEj.:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncor(data$cpi, data$hdi) # devuelve el valor de la correlación\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7238193\n```\n\n\n:::\n\n```{.r .cell-code}\ncor.test(data$cpi, data$hdi) # hace un test\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  data$cpi and data$hdi\nt = 13.186, df = 158, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6406049 0.7902298\nsample estimates:\n      cor \n0.7238193 \n```\n\n\n:::\n:::\n\n\n\nLos resultados muestran que la correlación entre ambas variables tiene un valor de 0,72. El test arroja tres resultados:\n\n-   t=13,186 --\\> t\\>3,26\n-   p\\<2.2e-16 --\\> p\\<0,001\n-   IC= \\[0.6406049, 0.7902298\\]\n\nBasándonos en estos resultados, podemos rechazar la hipótesis nula y afirmar que la correlación es significativamente distinta de cero para un nivel de confianza del 99,9%\n\n### **Correlación de más de dos variables a la vez:**\n\n\n\n\n\n\n\n1.  Correlación entre todas las variables del dataset. Es fundamental que sean TODAS numéricas.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncor(Data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            mpg        cyl       disp         hp        drat         wt\nmpg   1.0000000 -0.8521620 -0.8475514 -0.7761684  0.68117191 -0.8676594\ncyl  -0.8521620  1.0000000  0.9020329  0.8324475 -0.69993811  0.7824958\ndisp -0.8475514  0.9020329  1.0000000  0.7909486 -0.71021393  0.8879799\nhp   -0.7761684  0.8324475  0.7909486  1.0000000 -0.44875912  0.6587479\ndrat  0.6811719 -0.6999381 -0.7102139 -0.4487591  1.00000000 -0.7124406\nwt   -0.8676594  0.7824958  0.8879799  0.6587479 -0.71244065  1.0000000\nqsec  0.4186840 -0.5912421 -0.4336979 -0.7082234  0.09120476 -0.1747159\nvs    0.6640389 -0.8108118 -0.7104159 -0.7230967  0.44027846 -0.5549157\nam    0.5998324 -0.5226070 -0.5912270 -0.2432043  0.71271113 -0.6924953\ngear  0.4802848 -0.4926866 -0.5555692 -0.1257043  0.69961013 -0.5832870\ncarb -0.5509251  0.5269883  0.3949769  0.7498125 -0.09078980  0.4276059\n            qsec         vs          am       gear        carb\nmpg   0.41868403  0.6640389  0.59983243  0.4802848 -0.55092507\ncyl  -0.59124207 -0.8108118 -0.52260705 -0.4926866  0.52698829\ndisp -0.43369788 -0.7104159 -0.59122704 -0.5555692  0.39497686\nhp   -0.70822339 -0.7230967 -0.24320426 -0.1257043  0.74981247\ndrat  0.09120476  0.4402785  0.71271113  0.6996101 -0.09078980\nwt   -0.17471588 -0.5549157 -0.69249526 -0.5832870  0.42760594\nqsec  1.00000000  0.7445354 -0.22986086 -0.2126822 -0.65624923\nvs    0.74453544  1.0000000  0.16834512  0.2060233 -0.56960714\nam   -0.22986086  0.1683451  1.00000000  0.7940588  0.05753435\ngear -0.21268223  0.2060233  0.79405876  1.0000000  0.27407284\ncarb -0.65624923 -0.5696071  0.05753435  0.2740728  1.00000000\n```\n\n\n:::\n:::\n\n\n\n2.  Si solo se quiere la relación entre varias variables concretas, se puede hacer de forma manual del siguiente modo:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- Data[c(1:3, 5)]\ny <- Data[6:8]\ncor(x, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             wt        qsec         vs\nmpg  -0.8676594  0.41868403  0.6640389\ncyl   0.7824958 -0.59124207 -0.8108118\ndisp  0.8879799 -0.43369788 -0.7104159\ndrat -0.7124406  0.09120476  0.4402785\n```\n\n\n:::\n:::\n\n\n\n### **Visualización de correlaciones:**\n\nAdemás de usar funciones de cálculo, suele ser de gran ayuda visualizar las correlaciones entre variables gráficamente.\n\n-   Nube de puntos (dos variables):\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(data$cpi, data$hdi, #Los datos que se van a usar para hacer el gráfico\n     main = \"CPI/HDI\", #El título del gráfico\n     xlab = \"Corruption perception index\", #El texto del eje X\n     ylab = \"Human development index\", #El texto del eje Y\n     pch = 18) #Establece la forma de los puntos (triángulos, círculos, x...)\n```\n\n::: {.cell-output-display}\n![](42_correlacion_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\n-   Correlaciones para más de dos variables a la vez:\n\n``` r\nlibrary(corrgram)\ncorrgram(datos)\n```\n\nEj.:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncorrgram(Data)\n```\n\n::: {.cell-output-display}\n![](42_correlacion_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\n*El comando cuenta con numerosas argumentos extra para modificar y mejorar la visualización del gráfico final (ver ?corrgram). P. ej.:*\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncorrgram(Data, order=TRUE, lower.panel=NULL,\n         upper.panel=panel.pie, text.panel=panel.txt,\n         main=\"Car Milage Data in PC2/PC1 Order\")\n```\n\n::: {.cell-output-display}\n![](42_correlacion_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n## 2. Teoría\n\nSi queremos analizar la dependencia entre dos **variables continuas** XX e YY, no podemos estudiar sus distribuciones por separado, sino que debemos hacerlo de manera conjunta. Para ello, definimos una variable estadística bidimensional (X,Y)(X,Y), cuyos valores serán todos los pares formados por los valores de las variables XX e YY.\n\nLa representación gráfica más utilizada para examinar la relación entre dos variables numéricas es el diagrama de dispersión. Este consiste en representar, sobre un plano cartesiano, los puntos correspondientes a los pares de valores ($x_{i}$, $y_{i}$) de la variable bidimensional. Estas nubes de puntos nos permiten visualizar el tipo de relación existente entre las variables (lineal, exponencial, positiva, negativa, etc.). Si además queremos cuantificar la intensidad de dicha relación, es necesario recurrir a medidas estadísticas, como la covarianza muestral o el coeficiente de correlación.\n\nLa **covarianza** de una variable bidimensional se obtiene promediando los productos de las desviaciones de cada valor con respecto a las medias de XX e YY. Una vez calculadas las medias, podemos calcular la covarianza siguiendo la siguiente fórmula:\n\n$$cov_{x,y} = \\frac{\\sum\\limits_{i=1}^{n}{(x_i-\\overline{x}) \\cdot (y_i-\\overline{y})} }{n-1}$$\n\nEl valor de la covarianza nos indica lo siguiente:\n\n-   Si cov\\>0, relación lineal creciente entre las variables\n-   Si cov\\<0, relación lineal decreciente entre las variables\n-   Si cov=0, no existe relación lineal entre las variables\n\nEl problema de esta medida es que depende de las unidades. Imaginemos que las unidades de la variable x son *cm* y las de la variable y son *gr*. En este caso, las unidades de la covarianza serán *cm × gr*, y si cambiamos la escala de las variables, la covarianza también cambiará. Esto hace que el valor de la covarianza sea difícil de interpretar. (la variazna es la distancia de los puntos hacia los ejes. La covarianza es la distancia de los puntos entre sí)\n\nPara evitar este problema, es recomendable utilizar una medida normalizada, como el coeficiente de **correlación de Pearson**, que toma valores entre -1 y 1, donde:\n\n-   ρ = 1 indica una relación lineal perfecta y positiva\n-   ρ = -1 indica una relación lineal perfecta y negativa\n-   ρ = 0 indica ausencia de relación entre las variables\n\n$$\\rho = \\frac{\\text{cov}(X,Y)}{\\sigma_x \\sigma_y}$$\n\n**Correlación no implica causalidad**\n\nLa correlación entre dos variables v1 y v2 puede deberse a:\n\n-   Relación causal: V1 es la causa, V2 el efecto (o viceversa)\n-   Azar\n-   Variable interviniente (confounding factor):\n    -   [Relaciones espúreas](http://www.tylervigen.com/spurious-correlations): es una correlación aparente entre dos variables que en realidad es causada por la influencia de una tercera variable, conocida como variable de confusión. Aunque las dos variables parecen estar relacionadas, su relación no es causal.\n    -   [Paradoja de Simpson](https://upload.wikimedia.org/wikipedia/commons/f/fb/Simpsons_paradox_-_animation.gif): ocurre cuando una tendencia observada en varios grupos desaparece o se invierte al combinar los datos de esos grupos. Esto sucede debido a la influencia de una variable oculta o de confusión que afecta la interpretación de la relación entre las variables. Aquí tenéis un [ejemplo muy conocido](https://rpubs.com/dawnwp/1081716).\n\n------------------------------------------------------------------------\n",
    "supporting": [
      "42_correlacion_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}