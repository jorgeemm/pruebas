{
  "hash": "70238024edad0a00741b5a141265ecce",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Regresión logística\"\n---\n\n\n\n## Análisis de clasificación\n\nLa clasificación supervisada es una tarea muy frecuente en todas las áreas de análisis de datos. Existe un gran número de algoritmos desarrollados tanto por la estadística (regresión logística, análisis discriminante) como por la inteligencia artificial (redes neuronales, árboles de decisión, redes bayesianas) diseñados para realizar las tareas propias de la clasificación.\n\nVamos a centrarnos en el análisis de regresión logística, una técnica para el análisis de variables dependientes categóricas con dos categorías (**dicotómicas**) o más (**polinómicas**). Sirve para modelar la probabilidad de ocurrencia de un evento como función de otros factores, y responder preguntas como:\n\n-   ¿Qué factores explican la victoria/derrota de un candidato en unas elecciones?\n\n-   ¿Qué variables determinan que una persona vote?\n\n-   ¿Qué factores incrementan/disminuyen el riesgo de caer en la pobreza?\n\n-   ¿Cómo podemos explicar el abandono escolar?\n\n-   etc\n\nEl análisis de regresión logística pertenece al grupo de *Modelos Lineales Generalizados* (GLM por sus siglas en inglés), y usa como función de enlace la función *logit*.\n\n## Regresión logística vs regresión lineal\n\nEl modelo de regresión lineal no es válido cuando la variable respuesta no tiene una distribución normal. Por ejemplo: respuestas si/no, conteos, probabilidades, etc.\n\nAl igual que la regresión lineal, la regresión logística busca:\n\n-   predecir/explicar una VD a partir de una o mas VI,\n\n-   medir el grado de relación de la VD con las VI\n\n-   comprobar su significatividad\n\nA diferencia de la regresión lineal:\n\n-   los coeficientes de regresión se estiman por el procedimiento de **Máxima Verosimilitud**, que busca maximizar la probabilidad de ocurrencia del evento que se analiza\n\n## Supuestos básicos\n\n**Compartidos** con la Regresión Lineal:\n\n-   Tamaño muestral elevado\n\n-   Introducción de VIs relevantes\n\n-   Variables predictoras continuas o dicotómicas\n\n-   Ausencia de colinealidad entre las VIs\n\n-   Aditividad\n\n**Específicos**:\n\n-   No-linealidad: La función de vinculación logit es no-lineal. Esto implica que **el cambio en la VD producido por el incremento de una unidad en la VI depende del valor que tome la variable**. Es menos importante en los extremos de las VI, y mas importante en los valores centrales.\n\n![](images/clipboard-2977121164.png){fig-align=\"center\" width=\"300\"}\n\n-   Heterocedasticidad: En regresión logística se asume heterocedasticidad (varianza de los residuos no constante). Es lo contrario que en la regresión lineal, ya que la representación de la regresión logística no es lineal, sino que se busca que debe existir varianza en los residuos no constante.\n\n## La ecuación de la regresión logística\n\nLa función de enlace logit, utilizada principalmente en modelos de regresión logística, es como hemos mencionado no lineal, ya que transforma una combinación lineal de predictores en probabilidades mediante la fórmula:\n\n$$\n\\ln\\left(\\frac{p(x)}{1 - p(x)}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p\n$$\n\nEn esta ecuación, la VD aparece en una forma que no es directamente interpretable (concretamente, el logaritmo neperiano de la razón de probabilidades). Haciendo transformaciones, podemos expresar la probabilidad de ocurrencia del suceso de la siguiente manera:\n\n$$\np(x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p)}} = \\sigma\\left(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p\\right)\n$$\n\ndonde:\n\n$$\n\\sigma(x) = \\frac{e^x}{1 + e^x} = \\frac{1}{1 + e^{-x}}\n$$\n\n## Ejercicio de regresión logística\n\nVamos a hacer un sencillo ejercicio de regresión logística usando los datos del la encuesta preelectoral CIS 2023. En concreto, vamos a estimar la probabilidad de que un individuo *i* tenga intención de votar a un parido *p* en las elecciones generales de 2023. Dado que se trata de un ejercicio de clase, vamos a incluir unas pocas variables y no vamos a tomar en consideración los casos perdidos (indecisos, etc). En consecuencia, los resultados no sirven para predicción electoral, solo practicar.\n\nComo de costumbre, abrimos fichero “Limpieza de datos” en su versión más reciente.\n\nA partir de la variable intención de voto (´INTENCIONG\\`), vamos a crear nuestra variable dependiente de intención voto VOX (**intovox**).\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Intención de voto\ntable(datos$INTENCIONG)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   15   16   17 \n7200 7592 2652   36   21   86   89  250  170    3    3    7   12    8    1    9 \n  19   21   23   25  101  102  106  107  201  202  203  301  401  402  501  502 \n   3 2877    1    6   10   18    2    2    4   57    3    6    1    2   66   16 \n 601  801  802  803  804  808  809  814  901  902  903  904  905 1001 1101 1201 \n   7   37   37   10    1    4    1    1  296  230   17  108   15   75    1  281 \n1202 1301 1501 1502 1601 1602 1701 8993 8994 8995 8996 9977 9997 9998 9999 \n   2    3   30    8  233  302    1   27    7  106  376  226  700 4084  762 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Intención voto VOX\ndatos <- datos %>%\n  mutate(\n    intvox = case_when(\n      INTENCIONG == 3 ~ 1,            \n      INTENCIONG >= 9977 ~ NA,  \n      TRUE ~ 0))\n\ntable(datos$intvox, useNA = \"ifany\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n    0     1  <NA> \n20777  2652  5772 \n```\n\n\n:::\n:::\n\n\n\nComo variables independientes, vamos a utilizar las variables “edad”, “hombre”, “estudios_universitarios”, “ecoesp”, “ideol” y “recuerdo19” que ya tenemos preparadas de clases anteriores. Una vez tenemos todas las variables preparadas, procedemos a crear el data.frame **data** con el conjunto de variables que vamos a incluir nuestros análisis y eliminamos los casos perdidos.\n\n*Que existan tantos casos perdidos en la variable dependiente (5772) no importa en este caso al ser un ejemplo, pero a la hora de hacer un modelo de predicción real se deberían imputar todos estos valores para que el modelo sea más fiable*.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndatos_log <- datos %>%\n  dplyr::select(intvox, hombre, estudios_universitarios, edad, ecoesp, ideol, recuerdo19) %>%\n  drop_na()\n\nsummary(datos_log)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     intvox          hombre      estudios_universitarios      edad      \n Min.   :0.0000   Mujer : 9373   sin EU:10482            Min.   :21.00  \n 1st Qu.:0.0000   Hombre:10475   con EU: 9366            1st Qu.:41.00  \n Median :0.0000                                          Median :53.00  \n Mean   :0.1068                                          Mean   :52.78  \n 3rd Qu.:0.0000                                          3rd Qu.:65.00  \n Max.   :1.0000                                          Max.   :95.00  \n                                                                        \n      ecoesp          ideol             recuerdo19  \n negativa:12417   Min.   : 1.000   PSOE      :7104  \n positiva: 7431   1st Qu.: 3.000   PP        :4557  \n                  Median : 5.000   Otros     :2534  \n                  Mean   : 4.817   Podemos   :2292  \n                  3rd Qu.: 7.000   VOX       :1648  \n                  Max.   :10.000   Ciudadanos:1360  \n                                   (Other)   : 353  \n```\n\n\n:::\n:::\n\n\n\n## Estimación del modelo\n\nYa podemos estimar la regresión logística. De las 4 variables dependientes que tenemos, vamos a empezamos por calcular la probabilidad de votar a VOX frente a otros partidos, en función de la ideología, recuerdo de voto en 2019, opinión sobre la economía en España y perfil socio-demográfico de la persona. Usamos la función `glm()` con *link function* binominal.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(MASS)\nm.vox <- glm(intvox ~ hombre + estudios_universitarios + edad + ecoesp + ideol + recuerdo19, data = datos_log, family = \"binomial\")\nsummary(m.vox)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = intvox ~ hombre + estudios_universitarios + edad + \n    ecoesp + ideol + recuerdo19, family = \"binomial\", data = datos_log)\n\nCoefficients:\n                               Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                   -3.488087   0.164352 -21.223  < 2e-16 ***\nhombreHombre                   0.758371   0.064308  11.793  < 2e-16 ***\nestudios_universitarioscon EU -0.266082   0.063597  -4.184 2.87e-05 ***\nedad                          -0.027296   0.002093 -13.043  < 2e-16 ***\necoesppositiva                -1.352050   0.109386 -12.360  < 2e-16 ***\nideol                          0.290559   0.016197  17.939  < 2e-16 ***\nrecuerdo19PP                   0.478263   0.107785   4.437 9.11e-06 ***\nrecuerdo19VOX                  3.378650   0.109928  30.735  < 2e-16 ***\nrecuerdo19Podemos             -0.313977   0.206227  -1.522    0.128    \nrecuerdo19Ciudadanos           0.693385   0.127396   5.443 5.25e-08 ***\nrecuerdo19Más Madrid          -1.109060   1.011114  -1.097    0.273    \nrecuerdo19Otros               -0.266386   0.164990  -1.615    0.106    \nrecuerdo19En blanco           -0.123526   0.340733  -0.363    0.717    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 13484.3  on 19847  degrees of freedom\nResidual deviance:  7595.5  on 19835  degrees of freedom\nAIC: 7621.5\n\nNumber of Fisher Scoring iterations: 7\n```\n\n\n:::\n:::\n\n\n\n## Significatividad de las variables\n\nAl igual que en la regresión lineal, contrastamos las siguientes hipótesis:\n\n-   H~0~:βi=0 -\\> la VI~i~ no tiene efecto sobre la VD\n\n-   H~a~:βi!=0 -\\>la VI~i~ sí tiene efecto sobre la VD\n\nComo ya sabemos, podemos rechazar la hipótesis nula siempre que:\n\n-   p-valor\\<0.05, NC:95%\n\n-   p-valor\\<0.01, NC:99%\n\n-   p-valor\\<0.001, NC:99.9%\n\nSi además queremos calcular los intervalos de confianza, podemos usar la función `cofint()`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconfint(m.vox)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                    2.5 %      97.5 %\n(Intercept)                   -3.81281533 -3.16847185\nhombreHombre                   0.63278731  0.88491727\nestudios_universitarioscon EU -0.39093054 -0.14158952\nedad                          -0.03141038 -0.02320544\necoesppositiva                -1.57030106 -1.14124336\nideol                          0.25887651  0.32237807\nrecuerdo19PP                   0.26914384  0.69187062\nrecuerdo19VOX                  3.16575149  3.59683507\nrecuerdo19Podemos             -0.73570089  0.07550088\nrecuerdo19Ciudadanos           0.44317980  0.94295525\nrecuerdo19Más Madrid          -3.98404616  0.41077929\nrecuerdo19Otros               -0.59785450  0.05018137\nrecuerdo19En blanco           -0.85381772  0.49558218\n```\n\n\n:::\n:::\n\n\n\n## Interpretación de los coeficientes\n\nLos estimadores representan el logaritmo del cociente de probabilidades. Por ejemplo, ceteris paribus:\n\n-   Ideología: Para cada punto que aumenta la ideología, el log de la probabilidad de votar a VOX (versus votar otro partido) aumenta en 0.290559.\n\n-   Votó PP en 2019: el logaritmo de la probabilidad de votar a VOX en 2023 es 0.478263 mayor que entre los que en 2019 votaron al PP que entre los que votaron al PSOE (categoría de referencia).\n\nComo se puede observar, esta interpretación de los coeficientes es muy poco intuitiva. Este tipo de coeficiente es útil si lo que nos interesa es conocer la dirección del efecto (signo positivo o negativo) y el nivel de significación (p-valor). Si por el contrario estamos interesados en interpretar el valor coeficiente, tenemos dos alternativas mejores: expresar los coeficientes como *odds ratio* o calcular las probabilidades de ocurrencia del evento.\n\n## Odds ratio\n\nCon el *odds* *ratio* lo que hago es exponenciar el coeficiente, osea e exponencial de Beta: $e^{\\text{coef}}$ y es la frecuencia de ocurrencia de un suceso sobre la frecuencia de su no ocurrencia:\n\n-   Odds ratio \\>1: la variable tiene un efecto positivo sobre la probabilidad de ocurrencia del suceso.\n\n-   0 \\> Odds ratio \\<1: la variable tiene un efecto negativo sobre la probabilidad de ocurrencia del suceso.\n\n-   Odds ratio =1: la variable no tiene efecto sobre la probabilidad de ocurrencia del suceso.\n\nEn la tabla:\n\n-   Odds ratio~ideología~=e^0.290559^=1.337. Para cada punto que nos movemos a la derecha en la escala de ideología, la probabilidad de votar a VOX (sobre votar a otro partido) aumenta en un factor de 1.337 (cuando el resto de variables permanecen constantes). O si se quiere expresar en porcentaje.\n\n    -   (1,337 - 1)\\* 100 = 0,337 \\* 100 = 33,7%\n\n    Sale 1,33, el cual es el mismo valor del coeficiente solo que ahora lo calcula las betas elevadas a $e$ para desahacer el logaritmo neperiano.\n\n-   Odds ratio~votó PP(2019)~= e^0.478263^=1,61. La probabilidad de votar a VOX (sobre votar a otro partido) es 1,61 veces mayor entre las personas que en 2019 votaron al PP que entre las personas que en 2019 votaron al PSOE (cuando el resto de variables permanecen constantes). O si se quiere expresar en porcentaje.\n\n    -   (1,61 - 1)\\* 100 = 0,61 \\* 100 = 61%\n\n-   En el caso de que el coeficiente sea negativo, como por ejemplo “opinión sobre la situación de la economía en España” haríamos así: Odds ratio~ecoesp~=e^-1.352050^=0.259. En este caso, la probabilidad de votar a VOX es 0.259 veces menor entre las personas que consideran que la economía nacional va bien, que entre los que consideran que la economía nacional va mal (ceteris paribus). Expresado en porcentaje, sería:\n\n    -   (1 - 0.259)\\* 100 = 0.741 \\* 100 = 74%\n\nPara obtener los coeficientes exponenciados usamos la función `exp()`: Con este código le estoy pidiendo los odds ratio porque le estoy pidiendo el exponente de los coeficientes de la regresión logística.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nexp(coef(m.vox))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  (Intercept)                  hombreHombre \n                   0.03055927                    2.13479521 \nestudios_universitarioscon EU                          edad \n                   0.76637648                    0.97307287 \n               ecoesppositiva                         ideol \n                   0.25870935                    1.33717441 \n                 recuerdo19PP                 recuerdo19VOX \n                   1.61327017                   29.33114991 \n            recuerdo19Podemos          recuerdo19Ciudadanos \n                   0.73053605                    2.00047661 \n         recuerdo19Más Madrid               recuerdo19Otros \n                   0.32986896                    0.76614347 \n          recuerdo19En blanco \n                   0.88379885 \n```\n\n\n:::\n:::\n\n\n\nPara ponerlo todo en una tabla, usamos la función `cbind` (*column bind*), que nos permite unir la columna de los coeficientes y la de los intervalos de confianza.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nexp(cbind(OR = coef(m.vox), confint(m.vox)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                       OR       2.5 %      97.5 %\n(Intercept)                    0.03055927  0.02208591  0.04206783\nhombreHombre                   2.13479521  1.88285136  2.42278395\nestudios_universitarioscon EU  0.76637648  0.67642714  0.86797747\nedad                           0.97307287  0.96907780  0.97706174\necoesppositiva                 0.25870935  0.20798256  0.31942162\nideol                          1.33717441  1.29547382  1.38040656\nrecuerdo19PP                   1.61327017  1.30884340  1.99744850\nrecuerdo19VOX                 29.33114991 23.70655269 36.48258675\nrecuerdo19Podemos              0.73053605  0.47916950  1.07842418\nrecuerdo19Ciudadanos           2.00047661  1.55765237  2.56755800\nrecuerdo19Más Madrid           0.32986896  0.01861019  1.50799250\nrecuerdo19Otros                0.76614347  0.54999038  1.05146178\nrecuerdo19En blanco            0.88379885  0.42578629  1.64145359\n```\n\n\n:::\n:::\n\n\n\n**Comparar coeficientes**\n\nLos odds ratio se pueden comparar entre sí para saber qué variable es más explicativa o está asociada de manera más fuerte con la VD. Pero OJO! Para comparar un odds ratio mayor que uno (relación positiva) con un odds ratio menor que uno (relación negativa), es necesario calcular el valor inverso de uno de los datos porque el rango es distinto. Por ejemplo:\n\n-   ecoesp: 1/0,259=3,86\n\nCuando hacemos esto para poder comparar debemos tener en cuenta que este coeficiente es para la variable de referencia. En este caso 3,86 es para la economíanegativa, porque estamos dandole la vuelta al numerador y denominador.\n\nLa inversión de variables también es útil en el caso de las variables dicotómicas para comprobar el supuesto contrario al establecido. Por ejemplo, la probabilidad de las mujeres (en lugar del de los hombres) es de 1/2,13=0,47.\n\nSe puede hacer para cualquier tipo de variable, en el caso de la categóricas se invierte a su variable de referencias, y en el caso de las de escala (como la variable ideologia) lo entendemos como el aumento de una unidad.\n\n## Probabilidades predichas\n\nComo alternativa a los coeficientes y a los odds ratio se pueden calcular las probabilidades predichas. Las probabilidades predichas son la mejor manera de entender las variables de un modelo. Para calcularlas, primero debemos crear un *data.frame* con los valores que queremos que tomen las variables independientes en nuestras predicciones.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata1 <- with(datos_log, data.frame(ideol = mean(ideol), recuerdo19 = c(\"PSOE\",\"PP\",\"VOX\",\"Podemos\",\"Ciudadanos\", \"Más Madrid\", \"Otros\", \"En blanco\"), edad = mean(edad), hombre=\"Hombre\", estudios_universitarios=\"con EU\", ecoesp=\"negativa\"))\n\nhead(data1, 8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     ideol recuerdo19     edad hombre estudios_universitarios   ecoesp\n1 4.816505       PSOE 52.77877 Hombre                  con EU negativa\n2 4.816505         PP 52.77877 Hombre                  con EU negativa\n3 4.816505        VOX 52.77877 Hombre                  con EU negativa\n4 4.816505    Podemos 52.77877 Hombre                  con EU negativa\n5 4.816505 Ciudadanos 52.77877 Hombre                  con EU negativa\n6 4.816505 Más Madrid 52.77877 Hombre                  con EU negativa\n7 4.816505      Otros 52.77877 Hombre                  con EU negativa\n8 4.816505  En blanco 52.77877 Hombre                  con EU negativa\n```\n\n\n:::\n:::\n\n\n\nEste código predice con la media de ideología en la muestra en vez de con ideología = 5, para todos los recuerdos de voto, que esté en la media de edad de la muestra, que sea hombre, con estudios y con una opinión negativa para la economía.\n\nEs importante que las variables en este *data.frame* tengan el mismo nombre que las variables en la regresión logística anterior. Una vez creado el *data.frame*, ya podemos pedirle a R que calcule las probabilidades predichas.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata1$probpredichas_vox<- predict(m.vox, newdata = data1, type = \"response\")\ndata1[, c(2, 7)]  #le pido que muestre todas las filas de las columnas 2 (recuerdo voto) y 7(probabilidad predicha)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  recuerdo19 probpredichas_vox\n1       PSOE        0.04578266\n2         PP        0.07184266\n3        VOX        0.58459468\n4    Podemos        0.03386365\n5 Ciudadanos        0.08757578\n6 Más Madrid        0.01558029\n7      Otros        0.03545569\n8  En blanco        0.04067907\n```\n\n\n:::\n:::\n\n\n\nLos resultados muestran la probabilidad predicha de votar a VOX en 2023 para hombres con edad media, ideología media y con estudios universitarios, que opinan que la economía en España va mal, según el partido que habían votado en las elecciones anteriores.\n\n## Efectos marginales\n\nEl paquete *margins* responde a un intento de trasladar el comando “margins” de Stata a R, como un método genérico para calcular los efectos marginales -o efectos parciales- de las variables independientes. Por ejemplo, vamos calcular el efecto marginal de las VIs en nuestro modelo\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(margins)\nmargins_vox <- margins(m.vox)\n\n# Resumen\nsummary_margins <- summary(margins_vox) \nsummary_margins\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                        factor     AME     SE        z      p   lower   upper\n                ecoesppositiva -0.0621 0.0042 -14.6920 0.0000 -0.0704 -0.0538\n                          edad -0.0015 0.0001 -13.0236 0.0000 -0.0017 -0.0012\n estudios_universitarioscon EU -0.0142 0.0034  -4.1951 0.0000 -0.0208 -0.0076\n                  hombreHombre  0.0405 0.0034  11.9060 0.0000  0.0339  0.0472\n                         ideol  0.0155 0.0009  17.8011 0.0000  0.0138  0.0172\n          recuerdo19Ciudadanos  0.0365 0.0071   5.1327 0.0000  0.0225  0.0504\n           recuerdo19En blanco -0.0047 0.0124  -0.3784 0.7051 -0.0291  0.0197\n          recuerdo19Más Madrid -0.0289 0.0159  -1.8197 0.0688 -0.0601  0.0022\n               recuerdo19Otros -0.0096 0.0057  -1.6861 0.0918 -0.0208  0.0016\n             recuerdo19Podemos -0.0111 0.0067  -1.6524 0.0985 -0.0243  0.0021\n                  recuerdo19PP  0.0231 0.0049   4.6900 0.0000  0.0135  0.0328\n                 recuerdo19VOX  0.3882 0.0146  26.5553 0.0000  0.3596  0.4169\n```\n\n\n:::\n:::\n\n\n\nTambién podemos representarlos gráficamente. El gráfico a continuación representa la columna AME (*Average Marginal Effect*) y las columnas *low* and *upper* (bandas de confianza inferior y superior).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Convertimos el resumen en un data.frame para poder hacer un gráfico\ndata_to_plot <- data.frame(\n  factor = summary_margins$factor,\n  AME = summary_margins$AME,\n  lower = summary_margins$lower,\n  upper = summary_margins$upper)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data_to_plot, aes(x = AME, y = factor)) +\n  geom_point(color = \"blue\", size = 3) +  # Puntos para los AME\n  geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2, color = \"black\") +  # Barras de error\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"grey\") +  # Línea vertical en 0\n  labs(\n    title = \"Average Marginal Effects (AME) with Confidence Intervals\",\n    x = \"AME\",\n    y = \"Variables\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),  # Centrar título\n    axis.text.y = element_text(size = 10))  # Ajustar tamaño de texto\n```\n\n::: {.cell-output-display}\n![](54_reg_logistica_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\nInterpretación: Cuando se dice que un efecto marginal es 0.3882, significa que haber votado a VOX en 2019 está asociado con un aumento promedio de 0.17 unidades en la variable dependiente. En el contexto de una probabilidad o un porcentaje, como suele ser el caso en la regresión logística o modelos similares, un efecto marginal de 0.3882 corresponde a un aumento del 38,82 puntos porcentuales.\n\nPara hacer un modelo robusto puedes elegir poner las variables con sus valores que conformarían el escenario mas adverso. El escenario que más desfavorece la hipótesis planteada.\n\nPara más información sobre opciones del paquete margins podéis consultar [aquí](https://www.rdocumentation.org/packages/margins/versions/0.3.23) y [aquí](https://cran.r-project.org/web/packages/margins/vignettes/Introduction.html).\n\n**Resumen**\n\nLa primera transformación es pasar el logaritmo al otro lado, es decir $e^{\\text{coef}}$ –\\> los odds ratios, porque al otro lado del igual queda $(\\frac{p(x)}{1 - p(x)})$ , es decir la probabilidad del que el evento suceda partido de la probabilidad de que no suceda.\n\nLa última transfomación de la ecuación nos deja despejado $p(x)$, de esta manera podemos hacer probabilidades predichas de x sustituyendo los betas y las x (como hacías valores predichos en lm). Como la distribución de la muestra no es lineal los efectos de x no son iguales a lo largo de todo y, de esta manera podemos calcular casos específicos de y. Podemos conocer a lo largo de la regresión la probabilidad de y para cada x seleccionada por nosotros.\n\nPor último podemos calcular los *Average* *Marginal* *Effects* (*margins* en Stata). El efecto marginal no es la probabilidad de ocurrencia de x, sino el fecto de la VI sobre la probabilidad de ocurrencia de x. Es la manera de conseguir el equivalente a los coeficientes de la regresión lineal porque calcular todas las y para ciertos valores de x y luego hace el promedio de esos impactos de la x. En el caso de lineales no hace falta transformar nada porque la media del efectos de una VI sobre todos los valores de y es igual a el efecto de de VI sobre un solo valor de y porque es un efecto estable a lo largo de toda la regresión.\n\nAME se usa principalmente cuando tienes un interés particular por el efecto de una VI.\n\n## Diagnóstico\n\n#### Multicolinealidad\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rms)\nlogit.vif<- vif(m.vox)\nlogit.vif\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 hombreHombre estudios_universitarioscon EU \n                     1.032483                      1.048952 \n                         edad                ecoesppositiva \n                     1.077446                      1.058599 \n                        ideol                  recuerdo19PP \n                     1.428741                      2.968629 \n                recuerdo19VOX             recuerdo19Podemos \n                     2.498353                      1.189084 \n         recuerdo19Ciudadanos          recuerdo19Más Madrid \n                     1.726136                      1.006883 \n              recuerdo19Otros           recuerdo19En blanco \n                     1.295863                      1.061883 \n```\n\n\n:::\n:::\n\n\n\nTodos los valores están por debajo de 5. No parece que existan problemas de multicolinealidad\n\n#### Heterocedasticidad\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(lmtest)\nlogit.het<-bptest(m.vox)\nlogit.het\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tstudentized Breusch-Pagan test\n\ndata:  m.vox\nBP = 2273.9, df = 12, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\nLa hipótesis nula en este test es que la varianza de los residuos es constante. La evidencia permite rechazar la hipótesis nula, confirmando que se cumple el supuesto de heterocedasticidad (que es lo que buscamos).\n\n## Presentación de los resultados\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(stargazer)\n\nstargazer(m.vox,\n          type=\"text\",\n          dep.var.labels=c(\"Voto VOX\"),\n          covariate.labels=c(\"Hombre\", \"Estudios Universitarios\", \"Edad\", \"Valoración + economia\", \"Ideología\", \"Voto 2019: PP (cr:PSOE)\", \"Voto 2019: VOX (cr:PSOE)\", \"Voto 2019: Podemos (cr:PSOE)\", \"Voto 2019: Ciudadanos (cr:PSOE)\", \"Voto 2019: +Madrid (cr:PSOE)\", \"Voto 2019: Otros (cr:PSOE)\", \"Voto 2019: blanco (cr:PSOE)\", \"Constante\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n===========================================================\n                                    Dependent variable:    \n                                ---------------------------\n                                         Voto VOX          \n-----------------------------------------------------------\nHombre                                   0.758***          \n                                          (0.064)          \n                                                           \nEstudios Universitarios                  -0.266***         \n                                          (0.064)          \n                                                           \nEdad                                     -0.027***         \n                                          (0.002)          \n                                                           \nValoración + economia                    -1.352***         \n                                          (0.109)          \n                                                           \nIdeología                                0.291***          \n                                          (0.016)          \n                                                           \nVoto 2019: PP (cr:PSOE)                  0.478***          \n                                          (0.108)          \n                                                           \nVoto 2019: VOX (cr:PSOE)                 3.379***          \n                                          (0.110)          \n                                                           \nVoto 2019: Podemos (cr:PSOE)              -0.314           \n                                          (0.206)          \n                                                           \nVoto 2019: Ciudadanos (cr:PSOE)          0.693***          \n                                          (0.127)          \n                                                           \nVoto 2019: +Madrid (cr:PSOE)              -1.109           \n                                          (1.011)          \n                                                           \nVoto 2019: Otros (cr:PSOE)                -0.266           \n                                          (0.165)          \n                                                           \nVoto 2019: blanco (cr:PSOE)               -0.124           \n                                          (0.341)          \n                                                           \nConstante                                -3.488***         \n                                          (0.164)          \n                                                           \n-----------------------------------------------------------\nObservations                              19,848           \nLog Likelihood                          -3,797.747         \nAkaike Inf. Crit.                        7,621.493         \n===========================================================\nNote:                           *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n:::\n\n\n\n## Validación del modelos de clasificaicón\n\nA continuación, vamos examinar cómo de bueno/malo es nuestro modelo a la hora de clasificar datos nuevos. Para ello, continuamos con el ejemplo anterior, en el que estimábamos la probabilidad de que un individiuo vote a un determinado partido. Se llama clasficación porque estoy intentado clasificar a los individuos en función de los valores de la VD, que en este caso es 0 y 1.\n\nEn primer lugar, creamos el conjunto de entrenamiento (60%) y test (40%). Como hay pocos 1 en la muestra (de la var dependiente), se amplia el % de casos destinados al test para evitar que dentro de este la proporción de 1 sea demasiado bajo como para comprobar el modelo.Se saca una muestra aleatoria y por eso ponermos el seed, para que nos escoja los mismos datos aleatorios y sea replicable.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\nindex <- 1:nrow(datos_log)\nporc_test <- 0.40\n# Dividir datos\ntest.data <- datos_log %>% sample_frac(porc_test)  \ntrain.data <- datos_log %>% anti_join(test.data)\n```\n:::\n\n\n\nCreamos la variable **clase_real**, que corresponde a la variable dependiente (intentación voto a VOX) en el conjunto de test. Es la variable que luego comparemos con los valores estimados para ver nuestro nivel de acierto/error:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nclase_real <- test.data$intvox\n```\n:::\n\n\n\nEntrenamos el modelo (intención de voto a VOX) con los datos del train.data:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(MASS)\nlogit.vox <- glm(intvox ~ hombre + estudios_universitarios + edad + ecoesp + ideol + recuerdo19, data = train.data, family = \"binomial\")\nsummary(logit.vox)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = intvox ~ hombre + estudios_universitarios + edad + \n    ecoesp + ideol + recuerdo19, family = \"binomial\", data = train.data)\n\nCoefficients:\n                               Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                   -2.647381   0.223754 -11.832  < 2e-16 ***\nhombreHombre                   0.514395   0.091050   5.650 1.61e-08 ***\nestudios_universitarioscon EU -0.201434   0.089109  -2.261 0.023788 *  \nedad                          -0.019516   0.002758  -7.077 1.47e-12 ***\necoesppositiva                -1.382171   0.133390 -10.362  < 2e-16 ***\nideol                          0.203899   0.021920   9.302  < 2e-16 ***\nrecuerdo19PP                   0.952857   0.142989   6.664 2.67e-11 ***\nrecuerdo19VOX                  2.713181   0.147704  18.369  < 2e-16 ***\nrecuerdo19Podemos             -0.421267   0.245799  -1.714 0.086553 .  \nrecuerdo19Ciudadanos           0.585503   0.168807   3.468 0.000523 ***\nrecuerdo19Más Madrid          -1.237565   1.017536  -1.216 0.223895    \nrecuerdo19Otros               -0.600585   0.217137  -2.766 0.005676 ** \nrecuerdo19En blanco           -0.948500   0.526124  -1.803 0.071418 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5182.6  on 5941  degrees of freedom\nResidual deviance: 3442.7  on 5929  degrees of freedom\nAIC: 3468.7\n\nNumber of Fisher Scoring iterations: 6\n```\n\n\n:::\n:::\n\n\n\nDespués, calculamos los valores predichos en el conjunto de test. Estos son los valores que luego vamos a comparar con la “clase real”:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredicted_logit<- predict(logit.vox, newdata=test.data, type=\"response\")\nhead(predicted_logit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          1           2           3           4           5           6 \n0.006046023 0.024786792 0.050014734 0.009515188 0.213903071 0.064099475 \n```\n\n\n:::\n:::\n\n\n\nPara valorar cómo de bien/mal clasifica nuestro modelo, vamos a calcular la curva ROC(*Receiver operating characteristics*) y el AUC (*Area under the curve*). Cuando hacemos una clasificación binaria, existen 4 tipos de resultados posibles:\n\n-   True negative (TN): predecimos 0 y la clase real es 0\n\n-   False negative (FN): predecimos 0 y la clase real es 1\n\n-   True positive (TP): predecimos 1 y la clase real es 1\n\n-   False positive (FP): predecimos 1 y la clase real es 0\n\nA partir de estos resultados se construye la *matriz de confusión*:\n\n![](images/clipboard-2798392842.png){fig-align=\"center\" width=\"400\"}\n\nLa matriz de confusión sive para calcular la **curva ROC**, que es la representación gráfico de la razón de Verdaderos Positivos (TPR) frente a la razón de falsos positivos (FPR):\n\n-   Razón de verdaderos positivos (TPR - true positive rate): proporción de positivos reales que son correctamente identificados como positivos por el modelo. También se le conoce como sensibilidad o recall. Se calcula como:\n\n$$\nTPR = \\frac{TP}{TP + FN}\n$$\n\n-   Razón de falsos positivos (FPR - false positive rate) = proporción de negativos reales que son incorrectamente identificados como positivos por el modelo. Se calcula como:\n\n$$\nFPR = \\frac{FP}{FP + TN}\n$$\n\nGráficamente, el espacio ROC se representa de la siguiente manera:\n\n![](images/clipboard-2824448575.png){fig-align=\"center\" width=\"400\"}\n\nLo ideal es encontrar una curva que se acerque lo máximo posible al punto de clasificación perfecta. Si está en la línea, los resultados de la predicción son completamente aleatorios, y si está por debajo de esta el modelo predice aún peor que asignar cifras al azar.\n\nPintamos la curva ROC de nuestro modelo:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ROCR)\n\n# Curva ROC\npred_logit <-  prediction(predicted_logit, clase_real) # crea un objeto \"predicción\"\nperf_logit <- performance(pred_logit, measure = \"tpr\", x.measure = \"fpr\") \npar(mfrow = c(1,1))\nplot(perf_logit, lty=1, col=\"darkgrey\", main = \"Logit ROC Curve\")\n```\n\n::: {.cell-output-display}\n![](54_reg_logistica_files/figure-html/unnamed-chunk-20-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\nEsto se interpreta en función del area que hay debajo de la línea, lo ideal es que el area sea 1 (100%) es decir, que todo lo ha clasificado bien.\n\nCalculamos el area bajo la curva (AUC). El AUC (Área Bajo la Curva) es una métrica que mide el desempeño de un modelo de clasificación binaria. Un valor de AUC:\n\n-   1.0: El modelo clasifica perfectamente todas las instancias\n\n-   0.5: El modelo no tiene poder predictivo (es como adivinar al azar)\n\n-   \\< 0.5: El modelo clasifica peor que al azar.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nauc.logit<- performance(pred_logit, measure = \"auc\", x.measure = \"fpr\") \nauc.logit@y.values \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] 0.9201428\n```\n\n\n:::\n:::\n\n\n\nEn nuestro caso, AUC=0.920. Este valor indica que nuestro modelo clasifica bien en un 92% por de los casos.\n\nNota: AUC es un *S4 object system*, por eso las consultas de sus elementos son un poco diferentes a lo que hemos visto hasta ahora. Nosotros no vamos a entrar en esto, pero si teneís curiosidad podéis leer [este capítulo del libro de Hadley Wickham](http://adv-r.had.co.nz/S4.html).\n\n## Comparación de modelos\n\nNormalmente, la curva ROC se utiliza para comparar la precisión de diferentes algoritmos de clasificación (como regresión logística, Naive Bayes, Random Forest, LDA, etc). Aunque en nuestro caso solo hemos trabajado con regresión logística, vamos a estimar diferentes modelos a modo de ejemplo, pero sin profundizar en los detalles de su funcionamiento.\n\n### Modelo de clasificación Random Forest\n\nEmpezamos con un [random forest](https://www.r-bloggers.com/2021/04/random-forest-in-r/)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(randomForest)\n\n# Convertimos la variable intvoto en factor (de lo contrario, da error)\ntrain.data$intvox <- as.factor(train.data$intvox)\ntest.data$intvox <- as.factor(test.data$intvox)\n\n# Entrenamos el modelo Random Forest \nrf.vox <- randomForest(\n  intvox ~ hombre + estudios_universitarios + edad + ecoesp + ideol + recuerdo19,\n  data = train.data,\n  ntree = 500,      # número de árboles\n  mtry = 2,         # número de predictores seleccionados aleatoriamente por árbol\n  importance = TRUE # importancia de variables\n)\n\n# Importancia de las variables\nimportance(rf.vox) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                 0          1 MeanDecreaseAccuracy\nhombre                  12.8426166   9.205945             16.10155\nestudios_universitarios 17.3804480   1.960043             15.25825\nedad                    31.4921348  10.755326             31.06569\necoesp                   0.3002038  24.733429             24.64840\nideol                   29.2955504  20.302083             45.27504\nrecuerdo19              63.0086066 104.221899             96.01329\n                        MeanDecreaseGini\nhombre                          20.12300\nestudios_universitarios         19.83165\nedad                           157.91882\necoesp                          59.64442\nideol                          149.24749\nrecuerdo19                     374.84054\n```\n\n\n:::\n:::\n\n\n\nLe indicamos que haga 500 arboles; mtry es el número de predictores seecionados aleatoriamente. Importance indica que en los 500 arboles que variables han tenido más peso de manera sistemática en todos los modelos. Este valor no lo puedes interpretar pero te hace un ranking, cuanto más altos mejor (Accuracy y Gini). En vez de conocer la significatividad estadística tienemos la importancia de las variables en el modelo. El random forest va apartando poco a pode preditores que no sean relevantes para el modelo, según va haciendo los modelos de manera altearia.\n\nCuanto mayor es el número dentro del Accuracy, más relevante es esa variable dentro de la predicción. Que una variable sea más importante que otra no implica que sea mejor, ya que al no tener ninguna medida como el p-valor no se puede saber si la variable es significativa o no. Por tanto, que una variable sea más importante no la hace relevante en sí.\n\nEn este modelo el type se llama \"prob\" y luego si entre corchetes se pone \\[1\\] clasifica sobre los 0 y \\[,2\\] quiere decir que clasifica sobre los 1.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calculamos los valores predichos en el conjunto de test\npredicted_rf <- predict(rf.vox, newdata = test.data, type = \"prob\")[, 2]\n\n# Creamos el objeto de predicción para ROC\npred_rf <- prediction(predicted_rf, clase_real) # valores predichos, valores reales\n\n# Calculamos rendimiento (ROC)\nperf_rf <- performance(pred_rf, measure = \"tpr\", x.measure = \"fpr\")\n\n# Pintamos la curva ROC\npar(mfrow = c(1,1)) \nplot(perf_rf, lty = 1, col = \"gold\", main = \"Random Forest ROC Curve\")\n```\n\n::: {.cell-output-display}\n![](54_reg_logistica_files/figure-html/unnamed-chunk-23-1.png){fig-align='center' width=60%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calculamos el AUC\nauc.rf<- performance(pred_rf, measure = \"auc\", x.measure = \"fpr\") \nauc.rf@y.values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] 0.8862379\n```\n\n\n:::\n:::\n\n\n\nEn este caso, AUC es de 88,7%. Esto indica que en el 87% de los casos, el modelo clasifica correctamente.\n\n### Modelo de clasificación Naive Bayes\n\nRepetimos el proceso con el algoritmo [Naive Bayes](https://www.r-bloggers.com/2021/04/naive-bayes-classification-in-r/) (misma filosofía que un modelo bayesiano pero es otro modelo de clasificación).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(e1071)  \n\n# Entrenamos el modelo Naive Bayes \nnb_model <- naiveBayes(\n  intvox ~ hombre + estudios_universitarios + edad + ecoesp + ideol + recuerdo19, data = train.data)\n\n# Calculamos los valores predichos en el conjunto de test\npredicted_nb <- predict(nb_model, newdata = test.data, type = \"raw\")[, 2] # raw en esta librería equivale a prob en la de random.forest\n\n# Creamos el objeto de predicción para ROC\npred_nb <- prediction(predicted_nb, clase_real) #comparar datos predichos con reales\n\n# Calculamos el rendimiento (ROC)\nperf_nb <- performance(pred_nb, measure = \"tpr\", x.measure = \"fpr\")\n\n# Pintamos la curva ROC\npar(mfrow = c(1,1)) # Configuración de un solo gráfico\nplot(perf_nb, lty = 1, col = \"steelblue\", main = \"Naive Bayes ROC Curve\")\n```\n\n::: {.cell-output-display}\n![](54_reg_logistica_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=60%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calculamos el AUC\nauc.nb<- performance(pred_nb, measure = \"auc\", x.measure = \"fpr\") \nauc.nb@y.values \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] 0.9161327\n```\n\n\n:::\n:::\n\n\n\nEn este caso, el porcentaje de casos correctamente clasificados es 91,6%.\n\nPara tener una visión global vamos a representar todas las curvas ROC de manera conjunta. Esto nos permite comparar los resultados de los tres algoritmos:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npar(mfrow = c(1,1))\nplot(perf_logit, lty=1, col=\"darkgrey\", main = \"ROC Curves\")\nplot(perf_rf, lty=1, col=\"gold\", add = TRUE)\nplot(perf_nb, lty=1, col=\"steelblue\", add = TRUE)\nlegend(0.4, 0.6,  \n       c(\"Logit=0.920\", \"Random Forest=0.887\", \"Naive Bayes=0.916\"), \n       lty = c(1,1,1),       \n       bty = \"n\",\n       col=c(\"darkgrey\", \"gold\",\"steelblue\"),\n       cex = 0.7)\n```\n\n::: {.cell-output-display}\n![](54_reg_logistica_files/figure-html/unnamed-chunk-27-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\nEs importante recordar que nuestros modelos sólo están teniendo en consideración a los individuos que han respondido a todas las preguntas incluidas en el modelo. De esta manera, estamos dejando fuera a muchos entrevistados que no ofrecen información sobre alguna de las variables, particularmente, ideología y recuerdo. Esto genera un importante sesgo. ¿Sería mejor dejar estas variables fuera? A modo de ejemplo, vamos a ver cuál hubiera sido el resultado de no haber incluido información sobre esas variables. Repitamos nuestro modelo original, esta vez excluyendo ideología y recuerdo de voto en el 2019.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Modelo sin ideología ni recuerdo de voto\nlogit2.vox <- glm(intvox ~ hombre + estudios_universitarios + edad + ecoesp, data = train.data, family = \"binomial\")\n\n# Predicción\npredicted_logit2<- predict(logit2.vox, newdata=test.data, type=\"response\")\n\n# ROC\npred_logit2 <-  prediction(predicted_logit2, clase_real)\nperf_logit2<- performance(pred_logit2, measure = \"tpr\", x.measure = \"fpr\") \n\n# AUC\nauc.logit2 <- performance(pred_logit2, measure = \"auc\", x.measure = \"fpr\") \nauc.logit2@y.values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] 0.7653978\n```\n\n\n:::\n:::\n\n\n\nLa AUC en este caso es considerablemente más baja. Comparamos ambos modelos gráficamente:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npar(mfrow = c(1,1))\nplot(perf_logit, lty=1, col=\"darkgrey\", main = \"Logit ROC Curves\")\nplot(perf_logit2, lty=2, col=\"grey\", add = TRUE)\n\nlegend(0.4, 0.6,  \n       c(\"Logit=0.920\", \"Logit(sin ideología ni recuerdo de voto)=0.765\"), \n       lty = c(1,2),       \n       bty = \"n\",\n       col=c(\"darkgrey\", \"grey\"),\n       cex = 0.7)\n```\n\n::: {.cell-output-display}\n![](54_reg_logistica_files/figure-html/unnamed-chunk-29-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\nComo se puede observar en el gráfico arriba, la predicción es mucho peor cuando no disponemos de información sobre ideología y recuerdo de voto. Una alternativa sería imputar los valores perdidos en las variables que tienen un número elevado de NA antes de hacer la regresión.\n\n## Mejora de modelos\n\nVamos a explorar qué ocurre si tomamos en consideración que algunas variables pueden no tener un efecto lineal. Por ejemplo, la edad. Sabemos por otros estudios que el perfil de edad de los votantes de Vox es diverso, pero el partido tiene un respaldo importante entre votantes de mediana edad (35-54 años). Para tomar en cuenta esto, vamos a elevar ambos la variable edad al cuadrado. Recordad que siempre hay que incluir el efecto principal y el cuadrático\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogit3.vox <- glm(intvox ~ hombre + estudios_universitarios + edad + I(edad^2) + ecoesp + ideol + recuerdo19,\n  data = train.data, family = \"binomial\")\nsummary(logit3.vox)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = intvox ~ hombre + estudios_universitarios + edad + \n    I(edad^2) + ecoesp + ideol + recuerdo19, family = \"binomial\", \n    data = train.data)\n\nCoefficients:\n                                Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                   -3.9443595  0.4471310  -8.821  < 2e-16 ***\nhombreHombre                   0.5160163  0.0912237   5.657 1.54e-08 ***\nestudios_universitarioscon EU -0.2110982  0.0891749  -2.367 0.017921 *  \nedad                           0.0361276  0.0166679   2.167 0.030197 *  \nI(edad^2)                     -0.0005431  0.0001611  -3.372 0.000747 ***\necoesppositiva                -1.3912122  0.1335494 -10.417  < 2e-16 ***\nideol                          0.2082346  0.0220012   9.465  < 2e-16 ***\nrecuerdo19PP                   0.9575919  0.1427256   6.709 1.96e-11 ***\nrecuerdo19VOX                  2.6741532  0.1477779  18.096  < 2e-16 ***\nrecuerdo19Podemos             -0.4081878  0.2457836  -1.661 0.096761 .  \nrecuerdo19Ciudadanos           0.5629760  0.1688350   3.334 0.000855 ***\nrecuerdo19Más Madrid          -1.1715752  1.0172350  -1.152 0.249434    \nrecuerdo19Otros               -0.6173077  0.2172718  -2.841 0.004495 ** \nrecuerdo19En blanco           -0.9736638  0.5261391  -1.851 0.064230 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5182.6  on 5941  degrees of freedom\nResidual deviance: 3430.9  on 5928  degrees of freedom\nAIC: 3458.9\n\nNumber of Fisher Scoring iterations: 6\n```\n\n\n:::\n:::\n\n\n\nEn nuestro modelo, el efecto principal es positivo, y el cuadrático es negativo. Además, ambos son estadísticamente significativos. La combinación de estos efectos sugiere que la relación entre la variable independiente y la dependiente tiene forma de “U” invertida: La variable dependiente aumenta hasta cierto punto (el máximo) y luego comienza a disminuir.\n\nEn [este post](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faqhow-do-i-interpret-the-sign-of-the-quadratic-term-in-a-polynomial-regression/) se explica bastante bien la relación entre signos del efecto principal y cuadrático y la forma del efecto\n",
    "supporting": [
      "54_reg_logistica_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}